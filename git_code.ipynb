{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844166f2-149a-41f1-8376-b9681afb3213",
   "metadata": {},
   "source": [
    "# Code for bachelor's thesis\n",
    "## Data preprocessing\n",
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cd930-aa47-4132-95ee-146d88be1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('new_baseline.csv')\n",
    "\n",
    "sqrt = ['population_density', 'main_road_share', 'car_density', 'traff_score']\n",
    "log = ['bus_stops', 'main_road_density', 'all_road_density', 'bus_frequency', 'separated_frequency', 'bus_saturation', 'cars_per_road', 'separated_stops', 'bus_stop_density', 'separated_stop_density', 'cars_and_buses', 'population']\n",
    "\n",
    "# Apply square root transformation to selected columns\n",
    "for column in df.columns:\n",
    "    if column in sqrt:\n",
    "        df[column] = np.sqrt(df[column])\n",
    "        \n",
    "for column in df.columns:\n",
    "    if column in log:\n",
    "        df[column] = np.log(df[column])\n",
    "        \n",
    "df.to_csv('new_normed.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84fb6d7-1f9b-4d8e-9a84-a1b72c8acc00",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e52204-9021-4766-9bd7-9e68f62c5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data into a pandas DataFrame\n",
    "df = pd.read_csv('new_normed.csv')\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Create a new DataFrame with the scaled data\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "# Save the scaled data to a new CSV file\n",
    "scaled_df.to_csv('new_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f56a4b-c6d4-4147-bbf4-d9f57432c3a6",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94e66c-0565-4039-ae5d-bd555016f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load data into a pandas DataFrame\n",
    "df = pd.read_csv('new_scaled.csv')\n",
    "\n",
    "# Get a list of all variable names\n",
    "variable_names = df.columns.tolist()\n",
    "\n",
    "# Initialize empty DataFrame to store regression coefficients\n",
    "coefficients_df = pd.DataFrame(columns=variable_names, index=variable_names)\n",
    "\n",
    "# Loop through each pair of variables and run regression both ways\n",
    "for i in range(len(variable_names)):\n",
    "    for j in range(i+1, len(variable_names)):\n",
    "        var1 = variable_names[i]\n",
    "        var2 = variable_names[j]\n",
    "        # Create a list of all other variables in the dataset\n",
    "        other_vars = [v for v in variable_names if v not in [var1, var2]]\n",
    "        # Create a LinearRegression object and fit the data\n",
    "        model = LinearRegression().fit(df[[var2] + other_vars], df[var1])\n",
    "        # Store the regression coefficient for the independent variable\n",
    "        coefficients_df.loc[var1, var2] = model.coef_[0]\n",
    "        \n",
    "            # Create a LinearRegression object and fit the data\n",
    "        model = LinearRegression().fit(df[[var1] + other_vars], df[var2])\n",
    "        # Store the regression coefficient for the independent variable\n",
    "        coefficients_df.loc[var2, var1] = model.coef_[0]\n",
    "\n",
    "# Print the regression coefficients as a matrix\n",
    "print(coefficients_df)\n",
    "\n",
    "coefficients_df.to_csv('linear_weights.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2791185-1583-4702-86fd-1de108d5de64",
   "metadata": {},
   "source": [
    "### GlassoFCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a1f23-9bf0-4b08-bcef-8ef630501d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import GraphicalLasso\n",
    "import networkx as nx\n",
    "\n",
    "def glassoFCM(data, alpha):\n",
    "    # Initialize the GraphicalLasso model\n",
    "    model = GraphicalLasso(alpha=alpha, max_iter=1000, mode = 'cd')\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(data)\n",
    "    \n",
    "    # Get the inverse covariance matrix (precision matrix)\n",
    "    precision_matrix = model.precision_\n",
    "    \n",
    "    # Since we are only interested in the weights (and not the directions of the edges), \n",
    "    # we can convert the precision matrix to a correlation matrix\n",
    "    correlation_matrix = precision_matrix / np.sqrt(np.outer(np.diag(precision_matrix), np.diag(precision_matrix)))\n",
    "    \n",
    "    # The weights in the FCM are given by the elements of the correlation matrix\n",
    "    weights = correlation_matrix\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def invert_order(order):\n",
    "    # Get the inverted order\n",
    "    inverted_order = np.argsort(order)\n",
    "\n",
    "    return inverted_order\n",
    "\n",
    "def calculate_centrality_measures(weights):\n",
    "    # Create a directed graph from the weight matrix\n",
    "    G = nx.from_numpy_matrix(weights, create_using=nx.DiGraph)\n",
    "    \n",
    "    # Calculate the various centrality measures\n",
    "    strength_centrality = np.array(list(nx.degree_centrality(G).values()))\n",
    "    closeness_centrality = np.array(list(nx.closeness_centrality(G).values()))\n",
    "    betweenness_centrality = np.array(list(nx.betweenness_centrality(G).values()))\n",
    "    \n",
    "    return strength_centrality, closeness_centrality, betweenness_centrality\n",
    "\n",
    "def reorder_matrix(weights, order):\n",
    "    # Reorder the rows and columns of the weight matrix\n",
    "    weights_reordered = weights[order][:, order]\n",
    "    \n",
    "    return weights_reordered\n",
    "\n",
    "def glassoFCM_asymmetric(data, alpha):\n",
    "    # Get the symmetric weights using the glassoFCM function\n",
    "    weights = glassoFCM(data, alpha)\n",
    "    \n",
    "    # Calculate the various centrality measures\n",
    "    strength_centrality, closeness_centrality, betweenness_centrality = calculate_centrality_measures(weights)\n",
    "    \n",
    "    # Get the order of the nodes based on each centrality measure\n",
    "    order_strength = np.argsort(strength_centrality)\n",
    "    order_closeness = np.argsort(closeness_centrality)\n",
    "    order_betweenness = np.argsort(betweenness_centrality)    \n",
    "    \n",
    "    # Reorder the weight matrix based on each centrality measure\n",
    "    weights_strength = reorder_matrix(weights, order_strength)\n",
    "    weights_closeness = reorder_matrix(weights, order_closeness)\n",
    "    weights_betweenness = reorder_matrix(weights, order_betweenness)\n",
    "    \n",
    "    # Get the upper triangular part of each reordered weight matrix\n",
    "    weights_strength_upper = np.triu(weights_strength)\n",
    "    weights_closeness_upper = np.triu(weights_closeness)\n",
    "    weights_betweenness_upper = np.triu(weights_betweenness)\n",
    "    \n",
    "    # Get the inverted order for each centrality measure\n",
    "    inverted_order_strength = invert_order(order_strength)\n",
    "    inverted_order_closeness = invert_order(order_closeness)\n",
    "    inverted_order_betweenness = invert_order(order_betweenness)\n",
    "    \n",
    "    # Reorder the upper triangular weight matrix to the original order\n",
    "    weights_strength_final = reorder_matrix(weights_strength_upper, inverted_order_strength)\n",
    "    weights_closeness_final = reorder_matrix(weights_closeness_upper, inverted_order_closeness)\n",
    "    weights_betweenness_final = reorder_matrix(weights_betweenness_upper, inverted_order_betweenness)\n",
    "    \n",
    "    return weights_strength_final, weights_closeness_final, weights_betweenness_final\n",
    "\n",
    "df = pd.read_csv('new_scaled.csv')\n",
    "\n",
    "df['traffic_score'] = df['traff_score']\n",
    "df.drop('traff_score', axis=1, inplace=True)\n",
    "\n",
    "weights_strength, weights_closeness, weights_betweenness = glassoFCM_asymmetric(df.values, 0.01)\n",
    "weights_strength.to_csv('glasso_weights.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282fffa2-b0ff-49d8-9f35-f0b43e4e6473",
   "metadata": {},
   "source": [
    "# Running the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32071426-4111-4af7-a4c0-8256f47ecf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_w = pd.read_csv('glasso_weights.csv')\n",
    "df_w = df_w.fillna(0)\n",
    "\n",
    "def transfer_function(raw_activation_vector):\n",
    "    \"\"\"Re-scaled transfer function\"\"\"\n",
    "    norm = np.linalg.norm(raw_activation_vector)\n",
    "    if norm == 0:\n",
    "        return raw_activation_vector\n",
    "    else:\n",
    "        return raw_activation_vector / norm\n",
    "\n",
    "def quasi_nonlinear_reasoning_rule(weights, raw_activation_vector, initial_values, phi):\n",
    "    \"\"\"Quasi Nonlinear Reasoning Rule\"\"\"\n",
    "    new_activation_vector = []\n",
    "    for i in range(len(weights)):\n",
    "        temp = phi * np.dot(raw_activation_vector, weights[i]) + (1 - phi) * initial_values[i]\n",
    "        new_activation_vector.append(temp)\n",
    "    return np.array(new_activation_vector)\n",
    "\n",
    "def simulate_FCM(weights, initial_values, phi, iterations):\n",
    "    \"\"\"Simulate FCM\"\"\"\n",
    "    activation_vector = np.array(initial_values)\n",
    "    activation_values_history = [activation_vector]\n",
    "    for _ in range(iterations):\n",
    "        raw_activation_vector = np.dot(activation_vector, weights)\n",
    "        activation_vector = transfer_function(quasi_nonlinear_reasoning_rule(weights, raw_activation_vector, initial_values, phi))\n",
    "        activation_values_history.append(activation_vector)\n",
    "    return np.array(activation_values_history)\n",
    "\n",
    "weight_matrix = df_w.values\n",
    "\n",
    "\n",
    "# The names of your concepts\n",
    "concept_names = ['population_density', 'main_road_share', 'motorization_rate',\n",
    "       'car_density', 'all_road_density', 'traff_score', 'bus_frequency',\n",
    "       'separated_frequency', 'bus_saturation', 'bus_stop_density',\n",
    "       'separated_stop_density']\n",
    "\n",
    "# Initialize storage for the steady states\n",
    "steady_states = []\n",
    "\n",
    "# Range of initial values for the first concept\n",
    "init_values = np.arange(0.001, 1.1, 0.01)\n",
    "\n",
    "df_i = pd.read_csv('initial.csv')\n",
    "\n",
    "avg_activation_values = df_i.mean().values\n",
    "avg_activation_values = df_i.iloc[14] #Tilburg:56, Brussel:0, Charleroi: 6, Zurich:14\n",
    "init_activation_values = np.copy(avg_activation_values)\n",
    "\n",
    "changed = 0\n",
    "\n",
    "# Iterate over the range of initial values\n",
    "for init_value in init_values:\n",
    "    # Set the initial value of the first concept\n",
    "    init_activation_values[changed] = init_value\n",
    "    \n",
    "    # Run the simulation\n",
    "    activation_values = simulate_FCM(weight_matrix, init_activation_values, iterations=50, phi=0.1)\n",
    "    \n",
    "    # Store the steady state\n",
    "    steady_states.append(activation_values[-1])\n",
    "\n",
    "# Convert the list of steady states to a 2D numpy array\n",
    "steady_states = np.array(steady_states)\n",
    "\n",
    "# Create a plot for each concept\n",
    "for i, concept_name in enumerate(concept_names):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(init_values, steady_states[:, i])\n",
    "    plt.xlabel(\"Initial value of {}\".format(concept_names[changed]))\n",
    "    plt.ylabel(\"Steady state value of \"+concept_name)\n",
    "    plt.title(\"Change in steady state of \"+concept_name)\n",
    "    plt.tight_layout()\n",
    "    if i == 5:\n",
    "        plt.savefig('graphs/Zurich/{} v traff-score-zurich-glasso.pdf'.format(concept_names[changed]))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create a plot for each concept\n",
    "for i, concept_name in enumerate(concept_names):\n",
    "    plt.plot(init_values, steady_states[:, i], label=concept_name)\n",
    "\n",
    "plt.xlabel(\"Initial value of population_density\")\n",
    "plt.ylabel(\"Steady state value\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01130209-a6ff-416d-8532-7d9555fb866c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ba2de-76b8-4ce2-9011-5d94de0b50b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
